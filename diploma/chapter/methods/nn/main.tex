Нейронные сети представляют собой вычислительные модели, состоящие из узлов, называемых нейронами, организованных в слои. Каждый нейрон взвешивает входные сигналы, представленные как вектор \( \mathbf{x} = (x_1, x_2, ..., x_n) \), с весами \( \mathbf{w} = (w_1, w_2, ..., w_n) \) и смещением \( b \), где \( n \) - количество входов, \( x_i \) - \( i \)-й входной сигнал, \( w_i \) - весовой коэффициент \( i \)-го входа, \( b \) - смещение (bias). На выходе нейрона производится линейная комбинация входов с весами и смещением:

\[ z = \sum_{i=1}^{n} w_i x_i + b \]

Полученная сумма \( z \) затем подвергается нелинейному преобразованию при помощи функции активации \( f(z) \), которая определяет активацию нейрона:

\[ y = f(z) \]

Функция активации обычно вводится для добавления нелинейности в модель, что позволяет нейронной сети моделировать сложные нелинейные зависимости в данных. Некоторые из распространенных функций активации включают в себя сигмоидальную функцию (\( \sigma \)), гиперболический тангенс (\( \tanh \)), ReLU (Rectified Linear Unit) и их вариации.

В случае многослойной нейронной сети, выходы нейронов одного слоя становятся входами для следующего слоя, образуя цепочку преобразований. Процесс передачи данных через нейроны последовательных слоев называется прямым распространением (forward propagation).

Нейронные сети обучаются путем настройки весов \( \mathbf{w} \) и смещений \( b \) с использованием алгоритмов оптимизации, таких как градиентный спуск. Во время обучения модель минимизирует функцию потерь \( L \), которая оценивает разницу между предсказанным результатом и истинным значением:

\[ L = \frac{1}{N} \sum_{i=1}^{N} L(y_i, \hat{y}_i) \]

где \( N \) - количество обучающих примеров, \( y_i \) - истинное значение, \( \hat{y}_i \) - предсказанное значение.

