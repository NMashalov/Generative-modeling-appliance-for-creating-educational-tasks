############################################################
# pedagogy

@article{Egupova,
	title={Об основных требованиях, предъявляемых к задачам с прикладным содержанием в курсе школьной математики },
	author={Егупова М.В.},
	journal={Наука и школа.},
	volume={№3},
	pages={3210--3218},
}

@article{article,
author = {Bergviken-Rensfeldt, Annika and Rahm, Lina},
year = {2022},
month = {10},
pages = {1-19},
title = {Automating Teacher Work? A History of the Politics of Automation and Artificial Intelligence in Education},
volume = {5},
journal = {Postdigital Science and Education},
doi = {10.1007/s42438-022-00344-x}
}




@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

######################################################## 
#generative modeling


@misc{rombach2022highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{song2020generative,
      title={Generative Modeling by Estimating Gradients of the Data Distribution}, 
      author={Yang Song and Stefano Ermon},
      year={2020},
      eprint={1907.05600},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

################################################################
# neural nets

@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

#################################################################
# OCR

@inproceedings{kim2022ocr,
  title={Ocr-free document understanding transformer},
  author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle={European Conference on Computer Vision},
  pages={498--517},
  year={2022},
  organization={Springer}
}


@article{blecher2023nougat,
  title={Nougat: Neural optical understanding for academic documents},
  author={Blecher, Lukas and Cucurull, Guillem and Scialom, Thomas and Stojnic, Robert},
  journal={arXiv preprint arXiv:2308.13418},
  year={2023}
}


#################################################################
# computer vision


@article{lecun1989handwritten,
  title={Handwritten digit recognition with a back-propagation network},
  author={LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, Richard and Hubbard, Wayne and Jackel, Lawrence},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}


@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}


@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

############################################################
# nlp

@Article{Khurana2023,
author={Khurana, Diksha
and Koli, Aditya
and Khatter, Kiran
and Singh, Sukhdev},
title={Natural language processing: state of the art, current trends and challenges},
journal={Multimedia Tools and Applications},
year={2023},
month={Jan},
day={01},
volume={82},
number={3},
pages={3713-3744},
abstract={Natural language processing (NLP) has recently gained much attention for representing and analyzing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. In this paper, we first distinguish four phases by discussing different levels of NLP and components of Natural Language Generation followed by presenting the history and evolution of NLP. We then discuss in detail the state of the art presenting the various applications of NLP, current trends, and challenges. Finally, we present a discussion on some available datasets, models, and evaluation metrics in NLP.},
issn={1573-7721},
doi={10.1007/s11042-022-13428-4},
url={https://doi.org/10.1007/s11042-022-13428-4}
}


@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{NIPS2013_9aa42b31,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}


@inproceedings{heafield-2011-kenlm,
    title = "{K}en{LM}: Faster and Smaller Language Model Queries",
    author = "Heafield, Kenneth",
    editor = "Callison-Burch, Chris  and
      Koehn, Philipp  and
      Monz, Christof  and
      Zaidan, Omar F.",
    booktitle = "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2123",
    pages = "187--197",
}


###################################################################################
# sources

@online{libmipt,
  title={Электронная библиотека Физтеха},
  url={https://lib.mipt.ru/},
  urldate={2024-02-04}
}


@online{mathedu,
  title={Математическое образование общедоступная электронная библиотека},
  url={https://www.mathedu.ru/},
  urldate={2024-02-04}
}



@online{stablediffusion,
  title={Jay Alammar.The Illustrated Stable Diffusion},
  url={https://jalammar.github.io/illustrated-stable-diffusion/},
  urldate={2024-02-04}
}

